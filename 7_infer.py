'''
.pth æ˜¯ PyTorch çš„æ ‡å‡†æ¨¡å‹æƒé‡æ–‡ä»¶æ ¼å¼  ï¼Œä»…ä¿å­˜æ¨¡å‹çš„ state_dictï¼ˆå‚æ•°å­—å…¸ï¼‰ï¼Œä¸åŒ…å«æ¨¡å‹ç»“æ„å®šä¹‰ã€‚
ä¼˜ç‚¹ï¼šæ–‡ä»¶è¾ƒå°ï¼ŒåŠ è½½çµæ´»ï¼Œå¯è‡ªç”±é€‰æ‹©è®¾å¤‡ï¼ˆCPU/GPUï¼‰ã€‚
ç¼ºç‚¹ï¼šåŠ è½½æ—¶å¿…é¡»é‡æ–°å®ä¾‹åŒ–æ¨¡å‹ç»“æ„ï¼Œä¸”ä»£ç ä¸­çš„ç±»å®šä¹‰å¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ã€‚
é€‚ç”¨åœºæ™¯ï¼šè®­ç»ƒä¸æ¨ç†åˆ†ç¦»ï¼Œæˆ–éœ€è¦è·¨å¹³å°éƒ¨ç½²ã€‚

'''

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import numpy as np

# ==================== 1. å¤ç°æ¨¡å‹ç»“æ„ï¼ˆå¿…é¡»ä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰ ====================
class MultimodalRobotModel(nn.Module):
    def __init__(self, model_path, max_traj_len=50):
        super().__init__()
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )
        self.hidden_size = self.llm.config.hidden_size
        self.max_traj_len = max_traj_len
        
        # å›å½’å¤´
        self.regression_head = nn.Sequential(
            nn.Linear(self.hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, max_traj_len * 3)
        )
        self.regression_head.to(torch.bfloat16)

    def forward(self, input_ids, attention_mask, labels=None, **kwargs):
        outputs = self.llm(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True
        )
        
        hidden_states = outputs.hidden_states[-1]
        logits = outputs.logits
        
        # æå–ç”¨äºå›å½’çš„ç‰¹å¾ï¼šå–æœ€åä¸€ä¸ªæœ‰æ•ˆ token
        # è®­ç»ƒæ—¶é€»è¾‘ï¼šsequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = input_ids.shape[0]
        sequence_lengths = attention_mask.sum(dim=1) - 1
        
        reg_input = []
        for i in range(batch_size):
            idx = sequence_lengths[i]
            reg_input.append(hidden_states[i, idx])
        reg_input = torch.stack(reg_input)
        
        traj_flat = self.regression_head(reg_input)
        traj_pred = traj_flat.view(batch_size, self.max_traj_len, 3)
        
        loss_ce = None
        if labels is not None:
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_ce = nn.CrossEntropyLoss()(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        
        return loss_ce, traj_pred

# ==================== 2. æ¨ç†ç±»å°è£… ====================
class RobotTrajectoryPredictor:
    def __init__(self, model_path, checkpoint_path, max_traj_len=50, device='auto'):
        """
        Args:
            model_path: é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹è·¯å¾„
            checkpoint_path: å¾®è°ƒåçš„ .pth æƒé‡æ–‡ä»¶è·¯å¾„
            max_traj_len: æœ€å¤§è½¨è¿¹ç‚¹æ•°
            device: 'auto'è‡ªåŠ¨é€‰æ‹©ï¼Œæˆ–æŒ‡å®š 'cuda:0'/'cpu'
        """
        if device == 'auto':
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = torch.device(device)
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        self.model = MultimodalRobotModel(model_path, max_traj_len)
        self.model.to(self.device)
        self.model.eval()
        
        self._load_checkpoint(checkpoint_path)
        self.max_traj_len = max_traj_len
        
    def _load_checkpoint(self, checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        if list(checkpoint.keys())[0].startswith('module.'):
            checkpoint = {k[7:]: v for k, v in checkpoint.items()}
        
        missing, unexpected = self.model.load_state_dict(checkpoint, strict=False)
        if missing:
            print(f"âš ï¸  ç¼ºå¤±çš„æƒé‡: {missing}")
        if unexpected:
            print(f"âš ï¸  æ„å¤–çš„æƒé‡: {unexpected}")
        print(f"âœ… æ¨¡å‹æƒé‡å·²åŠ è½½: {checkpoint_path}")
    
    @torch.no_grad()
    def predict(self, instruction, max_new_tokens=512):
        """
        ä¸¤æ­¥æ¨ç†ï¼š
        1. ç”Ÿæˆæ–‡æœ¬ (Reasoning + Result)
        2. å›å½’è½¨è¿¹
        """
        # 1. æ„é€ è¾“å…¥ï¼ˆä»…æŒ‡ä»¤ï¼‰
        messages = [{"role": "user", "content": instruction}]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        # 2. ç”Ÿæˆæ–‡æœ¬
        # ä½¿ç”¨ model.llm.generate
        generated_ids = self.model.llm.generate(
            inputs.input_ids,
            max_new_tokens=max_new_tokens,
            do_sample=False, # è´ªå©ªè§£ç ï¼Œä¿è¯ç¡®å®šæ€§
            pad_token_id=self.tokenizer.pad_token_id
        )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆassistantçš„å›å¤ï¼‰
        # éœ€è¦å»æ‰è¾“å…¥éƒ¨åˆ†
        input_length = inputs.input_ids.shape[1]
        generated_tokens = generated_ids[0][input_length:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)

        # 3. å›å½’è½¨è¿¹
        # æ„é€ å…¨é‡æ–‡æœ¬ï¼šæŒ‡ä»¤ + ç”Ÿæˆçš„å›å¤
        full_messages = [
            {"role": "user", "content": instruction},
            {"role": "assistant", "content": generated_text}
        ]
        full_text = self.tokenizer.apply_chat_template(full_messages, tokenize=False, add_generation_prompt=False)
        full_inputs = self.tokenizer(full_text, return_tensors="pt").to(self.device)
        
        _, traj_pred = self.model(full_inputs.input_ids, full_inputs.attention_mask)
        
        # åå¤„ç†
        traj = traj_pred[0].cpu().numpy() # [max_traj_len, 3]
        valid_mask = traj[:, 2] > 0
        trajectory = traj[valid_mask]
        
        return trajectory, generated_text

# ==================== 3. ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    MODEL_PATH = "/home/lijia/code/LLaMA-Factory/models/Qwen/Qwen3-4B"
    CHECKPOINT_PATH = "robot_model_finetuned.pth"
    
    predictor = RobotTrajectoryPredictor(
        model_path=MODEL_PATH,
        checkpoint_path=CHECKPOINT_PATH,
        max_traj_len=50
    )
    
    test_instructions = [
        "Go to the LivingRoom and wait for 5 seconds",
        "Visit Kitchen then Bedroom in any order",
        "Move to RestRoom within 10 seconds"
    ]
    
    for instr in test_instructions:
        print(f"\nğŸ¤– æŒ‡ä»¤: {instr}")
        trajectory, response = predictor.predict(instr)
        # print(f"ğŸ“ æ¨¡å‹å›å¤: {response}")
        print(f"ğŸ“ ç”Ÿæˆè½¨è¿¹ç‚¹: {len(trajectory)} ä¸ª")
        print(f"ğŸ“Š è½¨è¿¹é¢„è§ˆ:\n{trajectory[:5]}")
